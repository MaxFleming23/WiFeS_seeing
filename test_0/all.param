ACQ_KEY = 'acq'
BGMED_KEY = 'BGMED'
BGRMSMED_KEY = 'BGRMSMED'
# SEGMPATH_KEY = 'SEGMPATH'
SEGMPATH_KEY = sextractor_checkimg_map['SEGMENTATION']
CONVPATH_KEY = 'CONVPATH'
# BGPATH_KEY = 'BGPATH'
BGPATH_KEY = sextractor_checkimg_map['BACKGROUND']
BGRMSPATH_KEY = sextractor_checkimg_map['BACKGROUND_RMS']
SEGMOBJ_KEY = 'SEGMOBJ'
PSF_CUTOUTS_PATH_KEY = 'PCUTPATH'
PSF_CUTOUTS_SIZE_KEY = 'PCUTSIZE'
NPSFPATH_KEY = 'NPSFPATH'
PIXSCALE_KEY = 'PIXSCALE'
ZP_MAD_KEY = 'ZPMAD'

sex_all_ground = CustomKernel(np.array([
    [1,2,1],
    [2,4,2],
    [1,2,1]
]))

def default_select_acquisition(
    images: ImageBatch,
) -> ImageBatch:
    """
    Returns images in a batch with are tagged as error

    :param images: set of images
    :return: subset of bias images
    """
    return select_from_images(images, key=OBSCLASS_KEY, target_values=ACQ_KEY)

def load_object(path):
    with open(path,'rb') as file:
        return pickle.load(file)
    
def dump_object(data,path):
    with open(path,'wb') as file:
        return pickle.dump(data,file)

def table_to_fake_image(table):
    return Image(data=np.zeros([1,1]),header=table.get_metadata())

class WifesAutoguiderVisier:
    
    def __init__(
        self,
        visier_catalog: VizierCatalog,
        cache: bool = False,
    ):
        self.visier_catalog = visier_catalog
        self.cache = cache
        
    def generator(
        self,
        image: Image,
    ) -> VizierCatalog | CatalogFromFile:
        
        logger.debug(image)
        filter_name = image["FILTER"]

        search_radius_arcmin = (
            np.max([image["NAXIS1"], image["NAXIS2"]])
            * np.max([np.abs(image["CD1_1"]), np.abs(image["CD1_2"])])
            * 60
        ) / 2.0
        
        # TODO: match closest
        if filter_name == 'I':
            filter_name = 'RP'

        return self.visier_catalog(
            min_mag=10,
            max_mag=20,
            search_radius_arcmin=search_radius_arcmin,
            filter_name=filter_name,
            cache_catalog_locally=self.cache,
        )
    
def wifes_autoguider_photometric_catalog_purifier(
    catalog: Table,
    image: Image
) -> Table:
    logger.debug('Using filter: wifes_autoguider_photometric_catalog_purifier')
    # TODO: filter
    # clean_mask = np.ones(catalog.to_pandas().shape,dtype=bool)
    # return catalog[clean_mask]
    
    
    return catalog

class PhotutilsBkgSubtractor(BaseImageProcessor):
    
    base_key = "photutilsbkgsubtractor"
    
    def __init__(
        self,
        box_size = 40,
        mask=None, 
        coverage_mask=None, 
        fill_value=0.0, 
        exclude_percentile=10.0, 
        filter_size=(3, 3), 
        filter_threshold=None, 
        edge_method='pad', 
        sigma_clip=SigmaClip(
            sigma=3.0, 
            sigma_lower=3.0, 
            sigma_upper=3.0,
            maxiters=10, 
            cenfunc='median', 
            stdfunc='std', 
            grow=False
        ), 
        bkg_estimator=SExtractorBackground(sigma_clip=None), 
        bkgrms_estimator=StdBackgroundRMS(sigma_clip=None),
        interpolator=BkgZoomInterpolator(),
        output_sub_dir = 'background',
        select_images: Callable[[ImageBatch], ImageBatch] = default_select_acquisition, #change
        dev: bool = False,
        cache: bool = False,
        save_bkg: bool = True,
        save_bkg_rms: bool = True,
    ):
        super().__init__()
        self.box_size = box_size
        self.mask = mask
        self.coverage_mask = coverage_mask
        self.fill_value = fill_value
        self.exclude_percentile = exclude_percentile
        self.filter_size = filter_size
        self.filter_threshold = filter_threshold
        self.edge_method = edge_method
        self.sigma_clip = sigma_clip
        self.bkg_estimator = bkg_estimator
        self.bkgrms_estimator = bkgrms_estimator
        self.interpolator = interpolator
        self.cache = cache
        self.output_sub_dir = output_sub_dir
        self.dev = dev
        self.select_images = select_images
        self.save_bkg = save_bkg
        self.save_bkg_rms = save_bkg_rms
    
    def _apply_to_images(
        self,
        batch: ImageBatch,
    ) -> ImageBatch:
        
        images = self.select_images(batch)
        
        for image in images:
            data = image.get_data()
            header = image.get_header()
            background = Background2D(
                data=data,
                box_size=self.box_size,
                mask = self.mask,
                coverage_mask = self.coverage_mask,
                fill_value = self.fill_value,
                exclude_percentile = self.exclude_percentile,
                filter_size = self.filter_size,
                filter_threshold = self.filter_threshold,
                edge_method = self.edge_method,
                sigma_clip = self.sigma_clip,
                bkg_estimator = self.bkg_estimator,
                bkgrms_estimator = self.bkgrms_estimator,
                interpolator = self.interpolator
            )
            background_map = background.background
            
            header[BGMED_KEY] = background.background_median
            header[BGRMSMED_KEY] = background.background_rms_median
            
            bkgsub = data - background_map
            image.set_data(bkgsub)
            
            save_images = {}
            output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
            
            if self.save_bkg:
                save_images[BGPATH_KEY] = 'background'
            
            if self.save_bkg_rms:
                save_images[BGRMSPATH_KEY] = 'background_rms'
            
            # TODO: 
            if self.cache:
                save_images += ['background']
                # bkg_image_name = image[BASE_NAME_KEY].replace('fits','background.fits')
                bkg_image_name = image[BASE_NAME_KEY]+'.background'
                header[BGPATH_KEY] = str(output_dir.joinpath(bkg_image_name))
                
            if self.dev:
                save_images[sextractor_checkimg_map['MINIBACKGROUND']] = 'background_mesh'
                save_images[sextractor_checkimg_map['MINIBACK_RMS']] = 'background_rms_mesh'
                save_name = image[BASE_NAME_KEY].replace('fits','background.pkl')
                dump_object(
                    data=background,
                    path=output_dir.joinpath(save_name)
                )
                
            for im in save_images.keys():
                # save_name = image[BASE_NAME_KEY].replace('fits',im+'.fits')
                save_name = image[BASE_NAME_KEY]+f".{save_images[im]}"
                save_path = output_dir.joinpath(save_name)
                save_to_path(
                    data=eval('background.'+save_images[im]),
                    header=image.header,
                    path=save_path,
                    overwrite=True
                )
                image[im] = str(save_path)
            
            image.set_header(header)
        
        return batch

class PhotutilsSourceFinder(BaseImageProcessor):
    """
    Processor to detect sources using photutils.segmentation.SourceFinder
    
    Args
        convolution_fwhm: FWHM of convolution mask
        convolution_kernel_size: size of convolution kernel
        npixels: number of connected pixels for detection
        threshold_factor: threshold factor of background RMS median
        connectivity: {4,8} source pixel grouping
        deblend: Whether to deblend overlapping sources.
        nlevels: The number of multi-thresholding levels to use for deblending.
        contrast: The fraction of the total source flux that a local peak must have 
            (at any one of the multi-thresholds) to be deblended as a separate object.
        mode: The mode used in defining the spacing between the multi-thresholding levels.
        relabel: If True (default), then the segmentation image will be relabeled after deblending
        nproc: The number of processes to use for multiprocessing (deblending)
        progress_bar: Whether to display a progress bar. 
        
    Returns
        ImageBatch
    """
    
    base_key = 'photutilssourcedetection'
    
    def __init__(
        self,
        output_sub_dir: Path | str = 'detection',
        convolve: bool = False,
        convolution_kernel: Kernel | None = None,
        convolution_fwhm: float | None = 2, 
        convolution_kernel_size: int | None = 3,
        npixels: int = 10, # default from SE config
        threshold_factor: float = 1.5, 
        connectivity: int = 8, # default from SE config
        deblend: bool = True,
        nlevels: int = 32, # default from SE config
        contrast: float = 0.001,
        mode: str = 'exponential', # default from SE config
        relabel: bool = True,
        nproc: int = 1,
        progress_bar: bool = False,
        dev: bool = False,
        cache: bool = False,
    ):
        super().__init__()
        self.cache = cache
        self.output_sub_dir = output_sub_dir
        self.npixels = npixels
        self.threshold_factor = threshold_factor
        self.connectivity = connectivity
        self.convolution_fwhm = convolution_fwhm
        self.convolution_kernel_size = convolution_kernel_size
        self.deblend = deblend
        self.contrast = contrast
        self.nlevels = nlevels
        self.mode = mode
        self.relabel = relabel
        self.nproc = nproc
        self.progress_bar = progress_bar
        self.dev = dev
        self.convolve = convolve
        self.convolution_kernel = convolution_kernel
    
    def _apply_to_images(
        self,
        batch: ImageBatch,
    ) -> ImageBatch:
        
        for image in batch:
            data = image.get_data()
            header = image.get_header()
            
            # convolve the data
            if self.convolve:
                if self.convolution_kernel is not None:
                    kernel = self.convolution_kernel
                else:
                    kernel = make_2dgaussian_kernel(
                        self.convolution_fwhm, 
                        size=self.convolution_kernel_size
                    )  
                convolved_data = convolve(data, kernel)
            else:
                convolved_data = data
            
            # detect the sources
            if BGRMSMED_KEY not in header.keys():
                raise PrerequisiteError(
                    f"{BGRMSMED_KEY} key not found in image. "
                    f"PhotutilsBkgSubtractor must be run before running this processor"
                )
            threshold = self.threshold_factor * image[BGRMSMED_KEY]  # per-pixel threshold
            finder = SourceFinder(
                npixels=self.npixels, 
                connectivity=self.connectivity,
                deblend = self.deblend,
                nlevels = self.nlevels,
                contrast = self.contrast,
                mode = self.mode,
                relabel = self.relabel,
                nproc=self.nproc,
                progress_bar=self.progress_bar,
            )
            segm = finder(convolved_data, threshold)
            
            if segm is None:
                # TODO: add logger message
                continue
            
            output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
            save_name = image[BASE_NAME_KEY]+'.segm'
            save_path = output_dir.joinpath(save_name)
            save_path_obj = str(save_path)+'.pkl' # maybe a different way?
            
            header[SEGMOBJ_KEY] = str(save_path_obj)
            header[SEGMPATH_KEY] = str(save_path)
            
            if self.convolve:
                save_path_conv = str(save_path).replace('segm','conv')
                header[CONVPATH_KEY] = str(save_path_conv)
            else:
                header[CONVPATH_KEY] = str(None)
            
            with open(save_path_obj, 'wb') as file:
                pickle.dump(segm, file) # better format?
            save_to_path(         # maybe move to dev as need only object not image
                data=segm.data,
                header=header,
                path=save_path,
                overwrite=True
            )
            save_to_path(
                data=convolved_data,
                header=header,
                path=save_path_conv,
                overwrite=True
            )
            
            if self.dev:
                params = ['cmap','polygons','segments','areas']
                for param in params:
                    with open(f"{save_path}.{param}.pkl", 'wb') as file:
                        pickle.dump(eval(f"segm.{param}"),file)

            image.set_header(header)
            
        return batch

#TODO: multiple aperture
class PhotutilsSourceCatalog(BaseSourceGenerator):
    """
    Args
        localbkg_width: The width of the rectangular annulus used to 
            compute a local background around each source.
        detection_cat: A SourceCatalog object for the detection image.
        make_cutouts: Whether to make cutouts for psf modeling
        cutout_size: size of cutout (if make_cutouts)
    
    Returns

    """
    
    base_key = "photutilssourcecatalog"
    
    def __init__(
        self, 
        calc_total_error: bool = False,
        error = None, # TODO: [somewhat done] maybe PhotutilsTotalErrorCalculator or calc_total_error internally
        mask = None,
        wcs = None, 
        localbkg_width = 15, # default: 0, mirar: 15
        background = None,
        use_background = False,
        apermask_method = 'correct', # default from SE config
        kron_params = [2.5, 3.5], # default from SE config
        detection_cat = None,
        progress_bar: bool = False,
        make_psf_cutouts: bool = True,
        psf_cutout_size: int = 21,
        output_sub_dir: str = "detection",
        copy_image_keywords: str | list[str] = None,
        cache: bool = False,
    ):    
        super().__init__()
        self.output_sub_dir = output_sub_dir
        self.copy_image_keywords = copy_image_keywords
        if isinstance(copy_image_keywords, str):
            self.copy_image_keywords = [self.copy_image_keywords]
        self.cache=cache
        self.error = error
        self.mask = mask
        self.wcs = wcs 
        self.localbkg_width = localbkg_width
        self.apermask_method = apermask_method
        self.kron_params = kron_params
        self.detection_cat = detection_cat
        self.progress_bar = progress_bar
        self.make_psf_cutouts = make_psf_cutouts
        self.psf_cutout_size = psf_cutout_size
        self.calc_total_error = calc_total_error
        self.background = background
        self.use_background = use_background
        
    def _apply_to_images(
        self,
        batch: ImageBatch,
    ) -> SourceBatch:
        
        src_batch = SourceBatch()
        
        for image in batch:
            
            data = image.get_data()
            header = image.get_header()
            
            # TODO: check pre-rec or as a processor function
            with open(header[SEGMOBJ_KEY],'rb') as file:
                segment_img = pickle.load(file)
            
            if self.calc_total_error: 
                if LATEST_WEIGHT_SAVE_KEY in header:
                    error = fits.getdata(image[LATEST_WEIGHT_SAVE_KEY])
                else:
                    error = None
            else:
                error = self.error
            
            if self.use_background:
                if self.background is not None:
                    background = fits.getdata(header[BGPATH_KEY]) # [!imp] restimate?
                else:
                    background = self.background
            else:
                background = None
                
            if self.wcs is None:
                wcs = WCS(header=header)
            else:
                wcs = self.wcs
            
            srccat = SourceCatalog(
                data=data,
                segment_img=segment_img,
                convolved_data=fits.getdata(header[CONVPATH_KEY]),
                error=error,
                mask=self.mask,
                background=background,
                wcs=wcs,
                localbkg_width=self.localbkg_width,
                apermask_method=self.apermask_method,
                kron_params = self.kron_params,
                detection_cat = self.detection_cat,
                progress_bar = self.progress_bar,
            )
            srccat_table = srccat.to_table()
            
            pix_scale = np.sqrt(np.abs(np.linalg.det(wcs.pixel_scale_matrix)))
            # TODO: maybe rename col instead of duplicate
            #       or make a table from scratch with only necessary columns
            srccat_table['NUMBER'] = srccat.label
            srccat_table['fwhm'] = srccat.fwhm
            srccat_table['ellipticity'] = srccat.ellipticity
            srccat_table['elong'] = srccat.elongation
            srccat_table[XPOS_KEY] = srccat.xcentroid
            srccat_table[YPOS_KEY] = srccat.ycentroid
            srccat_table['xcentroid_win'] = srccat.xcentroid_win
            srccat_table['ycentroid_win'] = srccat.ycentroid_win
            srccat_table['sky_centroid_icrs'] = srccat.sky_centroid_icrs
            srccat_table['sky_centroid_win'] = srccat.sky_centroid_win
            # logger.debug(srccat.sky_centroid_win)
            # cov_eigvals = srccat.covariance_eigvals
            # srccat_table['cov_eigvals'] = cov_eigvals
            # srccat_table['aimage'] = cov_eigvals[0]
            # srccat_table['bimage'] = cov_eigvals[1]
            srccat_table['aimage'] = srccat.semimajor_sigma
            srccat_table['bimage'] = srccat.semiminor_sigma
            srccat_table['THETA_IMAGE'] = srccat.orientation
            srccat_table['kron_aperture'] = srccat.kron_aperture
            
            # sex compatability
            srccat_table['ALPHAWIN_J2000'] = [coords.ra for coords in srccat.sky_centroid_win]
            srccat_table['DELTAWIN_J2000'] = [coords.dec for coords in srccat.sky_centroid_win]
            srccat_table['X_IMAGE'] = srccat.xcentroid_win
            srccat_table['Y_IMAGE'] = srccat.ycentroid_win
            srccat_table['FWHM_IMAGE'] = srccat.fwhm
            srccat_table['FWHM_WORLD'] = srccat.fwhm * pix_scale
            
            mag, mag_unc = get_mags_from_fluxes(
                flux_list = srccat.kron_flux,
                fluxunc_list = np.zeros(len(srccat_table)),
                zeropoint = 0.0,
                zeropoint_unc = 0.0,
            )
            srccat_table['MAG_AUTO'] = np.array(mag,dtype=float)
            
            # TODO: compute this somehow 
            aper_mags = {
            
            }
        
            if len(aper_mags) > 0:
                aper_fluxes = np.array(list(aper_mags.values()))
                mag, mag_unc = get_mags_from_fluxes(
                    flux_list = aper_fluxes,
                    fluxunc_list = np.zeros(aper_fluxes.shape),
                    zeropoint = 0.0,
                    zeropoint_unc = 0.0,
                )
                srccat_table['MAG_APER'] = np.array(mag,dtype=float)
            
            # TODO: CHANGE!!
            srccat_table['FLAGS'] = 0
            
            # mirar compatability
            srccat_table[DIFF_IMG_KEY] = image[LATEST_SAVE_KEY] # has to be 
            srccat_table[SCI_IMG_KEY] = '' #image[LATEST_SAVE_KEY]
            srccat_table[REF_IMG_KEY] = ''
            srccat_table[PIXSCALE_KEY] = pix_scale
    
            if self.make_psf_cutouts:
                # TODO: check if/what different for dithers with wcs
                
                # twiddle -->
                twiddle_keys = {
                    'label': 'id',
                    'xcentroid': 'x',
                    'ycentroid': 'y'
                }
                for key in twiddle_keys.keys():
                    srccat_table.rename_column(key,twiddle_keys[key])
                    
                stars = extract_stars( 
                    data=NDData(data=data),
                    catalogs=srccat_table[*twiddle_keys.values()],
                    size=self.psf_cutout_size
                )
                
                # twiddle <--
                for key in twiddle_keys.keys():
                    srccat_table.rename_column(twiddle_keys[key],key)
                
                save_name = image[BASE_NAME_KEY]+'.cutouts.pkl'
                output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
                save_path = output_dir.joinpath(save_name)
                with open(save_path,'wb') as file:
                    pickle.dump(stars,file)
                
                header[PSF_CUTOUTS_PATH_KEY] = str(save_path)
                header[PSF_CUTOUTS_SIZE_KEY] = self.psf_cutout_size
            
            output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
            output_cat = output_dir.joinpath(
                image[BASE_NAME_KEY].replace(".fits", ".cat")
            )
            
            header[SEXTRACTOR_HEADER_KEY] = str(output_cat)
            # header[CALSTEPS] += Sextractor.base_key
            save_table_as_ldac(
                tbl = srccat_table,
                file_path = output_cat
            )
            
            image.set_header(header)
            
            metadata = {}
            for key in image.keys():
                if key != "COMMENT":
                    metadata[key] = image[key]
            
            if len(aper_mags) > 0:
                srccat_table['MAG_APER'] = Table(aper_mags)
            src_batch.append(SourceTable(srccat_table.to_pandas(),metadata=metadata))
            
        return src_batch

class SourceCrossMatch(BaseProcessorWithCrossMatch):
    
    def __init__(
        self,
        ref_catalog_generator: Callable[[Image], BaseCatalog],
        temp_output_sub_dir: str = "phot",
        image_photometric_catalog_purifier: Callable[
            [Table, Image], Table
        ] = default_image_sextractor_catalog_purifier,
        crossmatch_radius_arcsec: float = 1.0,
        write_regions: bool = False,
        cache: bool = False,
        ref_cat_cols = None,
    ):
        super().__init__(
            ref_catalog_generator=ref_catalog_generator,
            temp_output_sub_dir=temp_output_sub_dir,
            crossmatch_radius_arcsec=crossmatch_radius_arcsec,
            sextractor_catalog_purifier=image_photometric_catalog_purifier,
            write_regions=write_regions,
            cache=cache,
            required_parameters=REQUIRED_PARAMETERS,
        )
        self.ref_cat_cols = ref_cat_cols
        
    def check_prerequisites(self):
        return True
        
    def _apply_to_images(
        self,
        batch: SourceBatch
    ) -> SourceBatch:
        
        batch_updated = SourceBatch()
        
        for table in batch:
            ref_cat, _, cleaned_img_cat = self.setup_catalogs(table_to_fake_image(table))
            matched_img_cat, matched_ref_cat, _ = self.xmatch_catalogs(
                ref_cat=ref_cat,
                image_cat=cleaned_img_cat,
                crossmatch_radius_arcsec=self.crossmatch_radius_arcsec, #2-3 arcsec
            )
            
            for cols in self.ref_cat_cols:
                matched_img_cat[cols] = matched_ref_cat[cols]
            
            matched_img_table = SourceTable(matched_img_cat.to_pandas(),metadata=table.metadata)
            batch_updated.append(matched_img_table)
            
        return batch_updated

class PhotutilsModelPSF(BaseSourceProcessor):
    
    base_key = 'photutils_model_psf'
    
    def __init__(
        self,
        oversampling=4, # don't oversample unless necessary*
        shape=None, # needs to be checked (edge pixles)
        shape_from_cutout: bool = False,
        smoothing_kernel='quartic', 
        recentering_func=centroid_com, 
        recentering_maxiters=20, 
        fitter=LMLSQFitter,
        fitter_params=dict(),
        fitter_kwargs=dict(),
        maxiters=10, 
        norm_radius=5.5, 
        recentering_boxsize=(5, 5), 
        center_accuracy=0.001, 
        sigma_clip=SigmaClip(
            sigma=3, 
            sigma_lower=3, 
            sigma_upper=3, 
            maxiters=10, 
            cenfunc='median', 
            stdfunc='std', 
            grow=False
        ),
        progress_bar=False, 
        fit_boxsize=5, # EPSFFitter default 
        psf_cutout_size: int = 21, 
        psf_cutout_size_auto: bool = False,
        make_psf_cutouts_from_catalog: bool = False,
        make_psf_cutouts: bool = True, # TODO: implement star detection/local peak finding to detect point-like objects specifically
        dev = False,
        output_sub_dir: Path | str = 'psf_model',
        cache: bool = False,
    ):
        super().__init__()
        self.cache=cache
        self.output_sub_dir = output_sub_dir
        self.oversampling=oversampling
        self.shape=shape
        self.smoothing_kernel=smoothing_kernel
        self.recentering_func=recentering_func
        self.recentering_maxiters=recentering_maxiters
        self.fitter=fitter
        self.fitter_params=fitter_params
        self.fitter_kwargs=fitter_kwargs
        self.maxiters=maxiters
        self.norm_radius=norm_radius
        self.recentering_boxsize=recentering_boxsize
        self.center_accuracy=center_accuracy
        self.sigma_clip=sigma_clip
        self.progress_bar=progress_bar
        self.fit_boxsize=fit_boxsize
        self.dev = dev
        self.psf_cutout_size = psf_cutout_size
        self.make_psf_cutouts = make_psf_cutouts
        self.make_psf_cutouts_from_catalog = make_psf_cutouts_from_catalog
        self.psf_cutout_size_auto = psf_cutout_size_auto
        
    def _apply_to_sources(
        self,
        batch: SourceBatch,
    ) -> SourceBatch:
        
        batch_updated = SourceBatch()
        
        for table in batch:
            
            data = table.get_data()
            metadata = table.get_metadata()
            image_data = fits.getdata(metadata[LATEST_SAVE_KEY])
            
            suffix = '.cutouts.pkl'
            fwhm_avg, fwhm_med, fwhm_std = sigma_clipped_stats(data['fwhm'])
            
            if self.psf_cutout_size_auto:
                psf_cutout_size = int(np.round(fwhm_avg) + 2*fwhm_std)*3
            else:
                psf_cutout_size = self.psf_cutout_size
            
            # update to use PSF_CUTOUTS_PATH_KEY
            # stars_file = Path(table[LATEST_SAVE_KEY]).parent.joinpath(table[BASE_NAME_KEY]+suffix)
            if PSF_CUTOUTS_PATH_KEY in metadata:
                stars_file = metadata[PSF_CUTOUTS_PATH_KEY]
                with open(stars_file,'rb') as file:
                    stars = pickle.load(file)
            
            elif self.make_psf_cutouts_from_catalog:
                twiddle_keys = {
                    'label': 'id',
                    'xcentroid': 'x',
                    'ycentroid': 'y'
                }
                # data.rename(twiddle_keys, axis='columns')
                
                stars_catalog = Table.from_pandas(data[list(twiddle_keys.keys())])
                for key in twiddle_keys.keys():
                    stars_catalog.rename_column(key,twiddle_keys[key])
                
                stars = extract_stars( 
                    data=NDData(data=image_data),
                    # catalogs=data[*twiddle_keys.values()],
                    catalogs=stars_catalog,
                    size=psf_cutout_size
                )
                
                # twiddle <--
                # data.rename({v: k for k, v in twiddle_keys.items()}, axis='columns')
                
                save_name = metadata[BASE_NAME_KEY]+'.cutouts.pkl'
                output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
                save_path = output_dir.joinpath(save_name)
                with open(save_path,'wb') as file:
                    pickle.dump(stars,file)
                
                metadata[PSF_CUTOUTS_PATH_KEY] = str(save_path)
                metadata[PSF_CUTOUTS_SIZE_KEY] = self.psf_cutout_size
        
            if self.fit_boxsize is None:
                fit_boxsize = int(np.round(fwhm_avg))*2
            else:
                fit_boxsize = self.fit_boxsize
                
            fitter = EPSFFitter(
                fitter=self.fitter(**self.fitter_params),
                fit_boxsize=fit_boxsize,
                **self.fitter_kwargs
            )
            
            epsf_builder = EPSFBuilder(
                oversampling=self.oversampling,
                shape=self.shape,
                smoothing_kernel=self.smoothing_kernel,
                recentering_func=self.recentering_func,
                recentering_maxiters=self.recentering_maxiters,
                fitter=fitter,
                maxiters=self.maxiters,
                norm_radius=self.norm_radius,
                recentering_boxsize=self.recentering_boxsize,
                center_accuracy=self.center_accuracy,
                sigma_clip=self.sigma_clip,
                progress_bar=self.progress_bar
            )
            epsf, fitted_stars = epsf_builder(stars)
            
            output_dir = get_output_dir(self.output_sub_dir, self.night_sub_dir)
            epsf_save_path = output_dir.joinpath(table[BASE_NAME_KEY].replace('fits','psfmodel.pkl'))
            with open(epsf_save_path,'wb') as file:
                pickle.dump(epsf, file)
                
            table[NPSFPATH_KEY] = str(epsf_save_path)
            
            if self.dev:
                # ? dev - maybe move to main
                data['psf_center_flat'] = np.full(data.shape[0],np.nan)
                for i,star in enumerate(fitted_stars.all_good_stars):
                    data.at[star.id_label-1,'psf_center_flat'] = str(fitted_stars.center_flat[i])
                
                # dev
                epsf_img_save_path = str(epsf_save_path).replace('pkl','fits')
                save_to_path(
                    data=epsf.data,
                    header=None,
                    path=epsf_img_save_path,
                    overwrite=True
                )
                
                save_suffix = suffix.replace('cutouts','cutouts.fitted')
                save_path = output_dir.joinpath(table[BASE_NAME_KEY]+save_suffix)
                with open(save_path,'wb') as file:
                    pickle.dump(fitted_stars, file)
            
            table.set_data(data)
            batch_updated.append(table)
            
        return batch_updated

def wifes_autoguider_gal_filter(
    batch: SourceBatch
) -> SourceBatch:
    batch_updated = SourceBatch()
    
    for table in batch:
        data = table.get_data()
        data = data[data['Gal'] == 0]
        batch_updated.append(SourceTable(data,metadata=table.metadata))
    
    return batch_updated  

class SourcePhotCalibrator(PhotCalibrator):
    
    def __init__(
        self,
        *args,
        **kwargs
    ):  
        super().__init__(*args,**kwargs)
        
    def get_sextractor_apertures(self) -> list[float]:
        # TODO: do it!
        return []
    
    def check_prerequisites(self):
        return True
    
    def table_to_fake_image(
        self,
        table
    ):
        return Image(data=np.zeros([1,1]),header=table.get_metadata())
    
    def calculate_zeropoint(
        self,
        ref_cat: Table,
        clean_img_cat: Table,
    ) -> list[dict]:
        """
        Function to calculate zero point from two catalogs
        Args:
            ref_cat: Reference catalog table
            clean_img_cat: Catalog of sources from image to xmatch with ref_cat
        Returns:
        """

        matched_img_cat, matched_ref_cat, _ = self.xmatch_catalogs(
            ref_cat=ref_cat,
            image_cat=clean_img_cat,
            crossmatch_radius_arcsec=self.crossmatch_radius_arcsec, #2-3 arcsec
        )
        logger.debug(
            f"Cross-matched {len(matched_img_cat)} sources from catalog to the image."
        )

        if len(matched_img_cat) < self.num_matches_threshold:
            err = (
                "Not enough cross-matched sources "
                "found to calculate a reliable zeropoint. "
                f"Only found {len(matched_img_cat)} crossmatches, "
                f"while {self.num_matches_threshold} are required. "
                f"Used {len(ref_cat)} reference sources and "
                f"{len(clean_img_cat)} image sources."
            )
            logger.error(err)
            raise PhotometryCrossMatchError(err)

        apertures = self.get_sextractor_apertures()  # aperture diameters
        zeropoints = []

        for i, aperture in enumerate(apertures):
            offsets = np.ma.array(
                matched_ref_cat["magnitude"] - matched_img_cat["MAG_APER"][:, i]
            )
            for outlier_thresh in self.outlier_rejection_threshold:
                cl_offset = sigma_clip(offsets, sigma=outlier_thresh)
                num_stars = np.sum(np.invert(cl_offset.mask))

                zp_mean, zp_med, zp_std = sigma_clipped_stats(
                    offsets, sigma=outlier_thresh
                )

                if num_stars > self.num_matches_threshold:
                    break

            check = [np.isnan(x) for x in [zp_mean, zp_med, zp_std]]
            if np.sum(check) > 0:
                err = (
                    f"Error with nan when calculating sigma stats: \n "
                    f"mean: {zp_mean}, median: {zp_med}, std: {zp_std}"
                )
                logger.error(err)
                raise PhotometryCalculationError(err)

            zp_mad = median_absolute_deviation(offsets)
            
            zero_dict = {
                "diameter": aperture,
                "zp_mean": zp_mean,
                "zp_median": zp_med,
                "zp_std": zp_std,
                "zp_mad": zp_mad,
                "nstars": num_stars,
                "mag_cat": matched_ref_cat["magnitude"][np.invert(cl_offset.mask)],
                "mag_apers": matched_img_cat["MAG_APER"][:, i][
                    np.invert(cl_offset.mask)
                ],
            }
            zeropoints.append(zero_dict)

        for outlier_thresh in self.outlier_rejection_threshold:
            offsets = np.ma.array(
                matched_ref_cat["magnitude"] - matched_img_cat["MAG_AUTO"]
            )
            cl_offset = sigma_clip(offsets, sigma=outlier_thresh)
            num_stars = np.sum(np.invert(cl_offset.mask))
            zp_mean, zp_med, zp_std = sigma_clipped_stats(offsets, sigma=outlier_thresh)
            zero_auto_mag_cat = matched_ref_cat["magnitude"][np.invert(cl_offset.mask)]
            zero_auto_mag_img = matched_img_cat["MAG_AUTO"][np.invert(cl_offset.mask)]

            if num_stars > self.num_matches_threshold:
                break
        
        zp_mad = median_absolute_deviation(offsets)
        
        zeropoints.append(
            {
                "diameter": "AUTO",
                "zp_mean": zp_mean,
                "zp_median": zp_med,
                "zp_std": zp_std,
                "zp_mad": zp_mad,
                "nstars": num_stars,
                "mag_cat": zero_auto_mag_cat,
                "mag_apers": zero_auto_mag_img,
            }
        )

        return zeropoints

    def apply_to_images(
        self,
        batch: ImageBatch,
    ) -> ImageBatch:
        phot_output_dir = self.get_phot_output_dir()
        phot_output_dir.mkdir(parents=True, exist_ok=True)

        for image in batch: 
            ref_cat, _, cleaned_img_cat = self.setup_catalogs(image)

            fwhm_med, _, fwhm_std, med_fwhm_pix, _, _ = get_fwhm(cleaned_img_cat)

            header_map = {
                "FWHM_MED": fwhm_med,
                "FWHM_STD": fwhm_std,
                "FWHM_PIX": med_fwhm_pix,
            }
            for key, value in header_map.items():
                if np.isnan(value):
                    value = -999.0
                image.header[key] = value

            if len(ref_cat) < self.num_matches_threshold:
                err = (
                    f"Not enough sources ({len(ref_cat)} found in reference catalog "
                    f"to calculate a reliable zeropoint. "
                    f"Require at least {self.num_matches_threshold} matches."
                )
                logger.error(err)
                raise PhotometryReferenceError(err)

            logger.debug(f"Found {len(cleaned_img_cat)} clean sources in image.")

            if len(cleaned_img_cat) < self.num_matches_threshold:
                err = (
                    f"Not enough sources ({len(cleaned_img_cat)} "
                    f"found in source catalog "
                    f"to calculate a reliable zeropoint. "
                    f"Require at least {self.num_matches_threshold} matches."
                )
                logger.error(err)
                raise PhotometrySourceError(err)

            zp_dicts = self.calculate_zeropoint(
                ref_cat=ref_cat, clean_img_cat=cleaned_img_cat
            )

            aperture_diameters = []
            zp_values = []

            with warnings.catch_warnings(record=True):
                warnings.simplefilter("ignore", category=VerifyWarning)

                for zpvals in zp_dicts:
                    image[f"ZP_{zpvals['diameter']}"] = zpvals["zp_mean"]
                    image[f"ZP_{zpvals['diameter']}_std"] = zpvals["zp_std"]
                    image[f"ZP_{zpvals['diameter']}_mad"] = zpvals["zp_mad"]
                    image[f"ZP_{zpvals['diameter']}_nstars"] = zpvals["nstars"]
                    try:
                        aperture_diameters.append(float(zpvals["diameter"]))
                        zp_values.append(zpvals["zp_mean"])
                    except ValueError:
                        continue

                aperture_diameters.append(med_fwhm_pix * 2)
                zp_values.append(image["ZP_AUTO"])

                if sextractor_checkimg_map["BACKGROUND_RMS"] in image.header.keys():
                    logger.debug(
                        "Calculating limiting magnitudes from background RMS file"
                    )
                    limmags = get_maglim(
                        image[sextractor_checkimg_map["BACKGROUND_RMS"]],
                        zp_values,
                        np.array(aperture_diameters) / 2.0,
                    )
                else:
                    limmags = [-99] * len(aperture_diameters)

                for ind, diam in enumerate(aperture_diameters[:-1]):
                    image[f"MAGLIM_{np.rint(diam)}"] = limmags[ind]
                image[MAGLIM_KEY] = limmags[-1]

                # [!] ERROR
                image[ZP_KEY] = image["ZP_AUTO"]
                # image[ZP_STD_KEY] = image["ZP_AUTO_STD"]
                image[ZP_STD_KEY] = image["ZP_AUTO_std"]
                # image[ZP_NSTARS_KEY] = image["ZP_AUTO_NSTARS"]
                image[ZP_MAD_KEY] = image["ZP_AUTO_mad"]
                image[ZP_NSTARS_KEY] = image["ZP_AUTO_nstars"]
                # [!] 
                image["MAGSYS"] = "AB"

        return batch
    
    def _apply_to_images(
        self,
        batch: SourceBatch,
    ) -> SourceBatch:
        
        batch_updated = SourceBatch()
    
        batch_fake_images = ImageBatch([self.table_to_fake_image(table) for table in batch])
        # batch_photcal = super()._apply_to_images(batch_fake_images) # if _apply_to_images in main file
        batch_photcal = self.apply_to_images(batch_fake_images)
        
        for table, table_updated in zip(batch,batch_photcal):
            table.metadata = table_updated.get_header()
            batch_updated.append(table)
            
        return batch_updated

class SeeingCalculator(BaseSourceProcessor):
    
    def __init__(
        self,
    ):
        super().__init__()
        
    def _apply_to_sources(
        self,
        batch: SourceBatch,
    ) -> SourceBatch:
        raise NotImplementedError
     
def load_wifes_guider_fits(
    path: str | Path
) -> tuple[np.array, astropy.io.fits.Header]:
    data, header = open_fits(path)
    header[OBSCLASS_KEY] = ACQ_KEY
    header[TARGET_KEY] = header['OBJECT']
    header[COADD_KEY] = 1
    header[GAIN_KEY] = 1
    header['CALSTEPS'] = ''
    header[PROC_FAIL_KEY] = ''
    if 'RADECSYS' in header:
        sys = header.pop('RADECSYS')
        header['RADESYSa'] = sys 
        header['RADECSYS'] = sys
    if 'FILTER' not in header:
        header['FILTER'] = header['TVFILT']
    return data, header

def load_wifes_guider_image(path: str | Path) -> Image:
    return open_raw_image(path, load_wifes_guider_fits)

load = [
    ImageLoader(input_sub_dir=RAW_DIR, input_img_dir=TEST_DIR, load_image=load_wifes_guider_image)
]

bkg_sub = [
    PhotutilsBkgSubtractor(
        box_size=(10,10),
        select_images=default_select_acquisition,
        output_sub_dir=OUTPUT_DIRS['BKG'],
        dev=False,
        save_bkg=False,
    ),
    ImageSaver(output_dir_name=OUTPUT_DIRS['BKG'])
]

src_det = [
    PhotutilsSourceFinder(
        convolve=True,
        convolution_kernel=sex_all_ground,
        output_sub_dir=OUTPUT_DIRS['DET'],
        dev=True
    ),
    PhotutilsSourceCatalog(
        make_psf_cutouts=False,
        use_background=False,
        output_sub_dir=OUTPUT_DIRS['DET']
    ),
    SourceWriter(output_dir_name=OUTPUT_DIRS['DET'])
]

xmatch = [
    SourceCrossMatch(
        ref_catalog_generator=WifesAutoguiderVisier(Gaia).generator,
        temp_output_sub_dir="phot",
        crossmatch_radius_arcsec=3.0, # or 2 TODO: test
        write_regions=True,
        cache=True,
        image_photometric_catalog_purifier=wifes_autoguider_photometric_catalog_purifier,
        ref_cat_cols=['Gal']
    ),
    CustomSourceTableModifier(modifier_function=wifes_autoguider_gal_filter),
    # SourceWriter(output_dir_name=OUTPUT_DIRS['XMAT'])
]

psfmodel = [
    PhotutilsModelPSF(
        # oversampling=1,
        make_psf_cutouts_from_catalog=True,
        dev = True,
        cache=True,
        output_sub_dir=OUTPUT_DIRS['PSF_MODEL'],
        fitter=TRFLSQFitter,
        # fitter_params={'calc_uncertainties':True},
        # fitter_kwargs=dict(),
        # shape=None,
    )
]

photcal = [
    SourcePhotCalibrator(
        ref_catalog_generator=WifesAutoguiderVisier(Gaia).generator,
        temp_output_sub_dir="phot",
        crossmatch_radius_arcsec=3.0, # or 2 TODO: test
        write_regions=True,
        cache=True,
        outlier_rejection_threshold=[1.5, 2.0, 3.0],
        image_photometric_catalog_purifier=wifes_autoguider_photometric_catalog_purifier,
        num_matches_threshold=0, # TODO: Change! (testing only ? maybe)
    ),
    SourceWriter(output_dir_name=OUTPUT_DIRS['PHOTCAL'])
]

test_config = list(itertools.chain(
    load,
    bkg_sub,
    src_det,
    # photcal
    xmatch,
    psfmodel
))

pipeline = WifesAutoguiderPipeline(night=f"test_{TEST_ID}")
pipeline.night_sub_dir = TEST_DIR
pipeline.add_configuration(configuration_name="test_config", configuration=test_config)

save_params(Path(TEST_DIR).joinpath('all.param'))